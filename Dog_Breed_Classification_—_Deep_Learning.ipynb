{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Dog Breed Classification — Deep Learning",
      "provenance": [],
      "mount_file_id": "1h1-5kY7RNdVJV3Pp2lE0T-MHNO8uQQU3",
      "authorship_tag": "ABX9TyNDOEjFCkJ42FU0kRdk5HnP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichaelUOFC/Dog-Breed-Classifier-Deep-Learning/blob/main/Dog_Breed_Classification_%E2%80%94_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy4v2EG9_PEg"
      },
      "source": [
        "from sklearn.datasets import load_files       \n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "from glob import glob"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8b0FhsX_3ze"
      },
      "source": [
        "# Step 1: Import Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_napKiR_wny"
      },
      "source": [
        "# define function to load train, test, and validation datasets\n",
        "def load_dataset(path):\n",
        "    data = load_files(path)\n",
        "    dog_files = np.array(data['filenames'])\n",
        "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
        "    return dog_files, dog_targets"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iDnwMpeDDBo"
      },
      "source": [
        "# load train, test, and validation datasets\n",
        "train_files, train_targets = load_dataset('/content/drive/MyDrive/dogImages/train')\n",
        "valid_files, valid_targets = load_dataset('/content/drive/MyDrive/dogImages/valid')\n",
        "test_files, test_targets = load_dataset('/content/drive/MyDrive/dogImages/valid')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCYLNV0EDFjM"
      },
      "source": [
        "# load list of dog names\n",
        "dog_names = [item[20:-1] for item in sorted(glob(\"/content/drive/MyDrive/dogImages/train/*/\"))]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "650ESU9TBb4Y"
      },
      "source": [
        "# Step 2: Detect Humans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNhykMMZ6wUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29979875-e04e-400a-d3b8-ddeee0bc9de9"
      },
      "source": [
        "import random\n",
        "random.seed(8675309)\n",
        "# load filenames in shuffled human dataset\n",
        "human_files = np.array(glob(\"/content/drive/MyDrive/lfw/*/*\"))\n",
        "random.shuffle(human_files)\n",
        "# print statistics about the dataset\n",
        "print('There are %d total human images.' % len(human_files))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 11146 total human images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQDRzBuoCFMS"
      },
      "source": [
        "# To detect a human face, we use OpenCV’s implementation of Haar feature-based cascade classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddmISedGB5zS"
      },
      "source": [
        "import cv2                \n",
        "import matplotlib.pyplot as plt                        \n",
        "%matplotlib inline\n",
        "# extract pre-trained face detector\n",
        "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
        "# returns \"True\" if face is detected in image stored at img_path\n",
        "def face_detector(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray)\n",
        "    return len(faces) > 0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNWlj00CCuGO"
      },
      "source": [
        "# Step 3: Detect Dogs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X_I1TWfCABp"
      },
      "source": [
        "from keras.applications.resnet50 import ResNet50\n",
        "# define ResNet50 model\n",
        "ResNet50_model = ResNet50(weights='imagenet')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Va82nQNC9Qu"
      },
      "source": [
        "Given how messy real world images could be, it normally requires some pre-processing before feeding into model. The path_to_tensor function below first resizes all images to a square that is 224×224 pixels (one of the key steps). Next, the image is converted to a 4D array (aka 4D tensor) since Keras CNN uses TensorFlow as the backend here. Tensor is a generalization of matrices to N-dimensional space. For more details, look at this great post by \n",
        "Matthew Mayo\n",
        ".\n",
        "Input shape: (nb_samples, rows, columns, channels) where,\n",
        "nb_samples is the total number of images (or samples), and\n",
        "rows, columns, and channels corresponds to the height, length, and depth of each image, respectively. Our 4D tensor would be (1, 224, 224, 3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kKH3r6xClTC"
      },
      "source": [
        "from keras.preprocessing import image                  \n",
        "from tqdm import tqdm\n",
        "def path_to_tensor(img_path):\n",
        "    # loads RGB image as PIL.Image.Image type\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
        "    x = image.img_to_array(img)\n",
        "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
        "    return np.expand_dims(x, axis=0)\n",
        "def paths_to_tensor(img_paths):\n",
        "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
        "    return np.vstack(list_of_tensors)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYo512O3DNK4"
      },
      "source": [
        "There are a few additional preprocessing steps required before the model can be used for prediction like convert the RGB image to BGR, normalize models by subtracting the mean pixel from every pixel in each image etc. This is all taken care of by the preprocess_input function. To know more about it, check out code here.\n",
        "Now that our image is formatted, we are ready to supply it to ResNet-50 and make predictions. The predict function below returns the predicted probability of the image belonging to a specific ImageNet category. For mapping the returned integers to the model’s predicted object class, please use this dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3Xm2P6iDHlK"
      },
      "source": [
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "def ResNet50_predict_labels(img_path):\n",
        "    # returns prediction vector for image located at img_path\n",
        "    img = preprocess_input(path_to_tensor(img_path))\n",
        "    return np.argmax(ResNet50_model.predict(img))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_rZ2ck0DToi"
      },
      "source": [
        "All the dog categories in the dictionary correspond to keys 151–268. Therefore, to detect a dog face, we need to check if ResNet50_predict_labels function below returns a value between 151 and 268 (inclusive). The function returns a True if a dog face is detected, and False otherwise.\n",
        "Similar to step 2, we run a sample of 100 images through this function. Our model was able to detect a dog face in 100% of dog images and 0% in human images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LugtEdWaDmDY"
      },
      "source": [
        "# # Step 4: Create a CNN to Classify Dog Breeds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPvUdBykDdP6"
      },
      "source": [
        "Now that we are able to detect a human and a dog face in the images, our next goal is to classify the dog breed. We created a CNN model to help with these classification predictions. Before we start building the model, we rescaled the images by dividing every pixel in every image by 255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-32BQQqgF0jw"
      },
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtdikkF4DiWv",
        "outputId": "9652e3fc-21a3-4b7d-acd8-96647a13256d"
      },
      "source": [
        "# pre-process the data for Keras\n",
        "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
        "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
        "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6714/6714 [00:57<00:00, 117.53it/s]\n",
            "100%|██████████| 835/835 [00:06<00:00, 129.22it/s]\n",
            "100%|██████████| 835/835 [00:06<00:00, 132.27it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQUAlbtDtUk"
      },
      "source": [
        "Below is the CNN architecture from our classification model. CNNs are often designed with a goal of making the input arrays much deeper than its length or width. In the 3 Convolutional layers used below, I am increasing the number of filters to increase the stack of features, and thus increase its depth. Each convolutional layer is followed by a MaxPooling layer, to reduce the spatial dimensionality of the image. We then flatten the matrices into a vector, before we feed it into a fully connected Dense layer since these do not accept multidimensional arrays. This layer uses a softmax activation function to get the classification probability of each category, and 133 output nodes, 1 for each dog category in our training data. To go into detail of each parameter, and how these are calculate, please see this post by \n",
        "Rakshith Vasudev\n",
        ".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T9VKycADyu4",
        "outputId": "3bbf8120-cb35-46da-b9b0-75eddd450290"
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "from keras.models import Sequential\n",
        "# Define your architecture.\n",
        "model = Sequential()\n",
        "# Convolutional layers and maxpooling layers, note: all images are 224*224 pixel\n",
        "model.add(Conv2D(filters=16, kernel_size=2, strides=1, padding='same',activation='relu', input_shape=[224,224,3]))\n",
        "model.add(MaxPooling2D(pool_size=2, strides=1, padding='same'))\n",
        "model.add(Conv2D(filters=32, kernel_size=2, strides=2, padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2, strides=1, padding='same'))\n",
        "model.add(Conv2D(filters=64, kernel_size=2, strides=2, padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2, strides=1, padding='same'))\n",
        "#model.add(GlobalAveragePooling2D())\n",
        "# Flatten the array into a vector and feed to a dense layer\n",
        "model.add(Flatten())\n",
        "model.add(Dense(133, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 224, 224, 16)      208       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 224, 224, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 112, 112, 32)      2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 56, 56, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 200704)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 133)               26693765  \n",
            "=================================================================\n",
            "Total params: 26,704,309\n",
            "Trainable params: 26,704,309\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHeZpq3bZia5"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
        "                               verbose=1, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvKyKNgYEfM5"
      },
      "source": [
        "Next, we compile, and train our model. ModelCheckpoint is used to save the model that attains the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pMikVmpIMQt",
        "outputId": "b9bd550f-7609-459d-e014-23c15bd6bdbd"
      },
      "source": [
        "model.fit(train_tensors, train_targets, \n",
        "          validation_data=(valid_tensors, valid_targets), epochs= 50, batch_size=20, callbacks=[checkpointer], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "336/336 [==============================] - 45s 36ms/step - loss: 5.1113 - accuracy: 0.0180 - val_loss: 4.5516 - val_accuracy: 0.0539\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.55164, saving model to saved_models/weights.best.from_scratch.hdf5\n",
            "Epoch 2/100\n",
            "336/336 [==============================] - 11s 34ms/step - loss: 3.9467 - accuracy: 0.1599 - val_loss: 4.4822 - val_accuracy: 0.0838\n",
            "\n",
            "Epoch 00002: val_loss improved from 4.55164 to 4.48218, saving model to saved_models/weights.best.from_scratch.hdf5\n",
            "Epoch 3/100\n",
            "336/336 [==============================] - 12s 34ms/step - loss: 1.6075 - accuracy: 0.6532 - val_loss: 5.9546 - val_accuracy: 0.0886\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 4.48218\n",
            "Epoch 4/100\n",
            "336/336 [==============================] - 11s 34ms/step - loss: 0.3243 - accuracy: 0.9338 - val_loss: 8.8832 - val_accuracy: 0.0814\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 4.48218\n",
            "Epoch 5/100\n",
            "336/336 [==============================] - 12s 34ms/step - loss: 0.1099 - accuracy: 0.9805 - val_loss: 10.5356 - val_accuracy: 0.0778\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 4.48218\n",
            "Epoch 6/100\n",
            "336/336 [==============================] - 12s 34ms/step - loss: 0.0563 - accuracy: 0.9875 - val_loss: 11.7645 - val_accuracy: 0.0754\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 4.48218\n",
            "Epoch 7/100\n",
            "336/336 [==============================] - 12s 34ms/step - loss: 0.0618 - accuracy: 0.9887 - val_loss: 25.2630 - val_accuracy: 0.0563\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 4.48218\n",
            "Epoch 8/100\n",
            "336/336 [==============================] - 12s 34ms/step - loss: 0.0621 - accuracy: 0.9923 - val_loss: 16.7452 - val_accuracy: 0.0407\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 4.48218\n",
            "Epoch 9/100\n",
            "336/336 [==============================] - 12s 34ms/step - loss: 0.0264 - accuracy: 0.9953 - val_loss: 18.8461 - val_accuracy: 0.0659\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 4.48218\n",
            "Epoch 10/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0199 - accuracy: 0.9982 - val_loss: 16.3076 - val_accuracy: 0.0719\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 4.48218\n",
            "Epoch 11/100\n",
            "336/336 [==============================] - 12s 34ms/step - loss: 0.0197 - accuracy: 0.9980 - val_loss: 14.0639 - val_accuracy: 0.0695\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 4.48218\n",
            "Epoch 12/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0332 - accuracy: 0.9961 - val_loss: 16.4721 - val_accuracy: 0.0635\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 4.48218\n",
            "Epoch 13/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0160 - accuracy: 0.9976 - val_loss: 17.9683 - val_accuracy: 0.0635\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 4.48218\n",
            "Epoch 14/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0199 - accuracy: 0.9974 - val_loss: 17.2232 - val_accuracy: 0.0671\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 4.48218\n",
            "Epoch 15/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0227 - accuracy: 0.9969 - val_loss: 13.9045 - val_accuracy: 0.0743\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 4.48218\n",
            "Epoch 16/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0223 - accuracy: 0.9983 - val_loss: 14.8931 - val_accuracy: 0.0575\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 4.48218\n",
            "Epoch 17/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0211 - accuracy: 0.9975 - val_loss: 16.4979 - val_accuracy: 0.0671\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 4.48218\n",
            "Epoch 18/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0179 - accuracy: 0.9984 - val_loss: 14.9876 - val_accuracy: 0.0587\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 4.48218\n",
            "Epoch 19/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0272 - accuracy: 0.9974 - val_loss: 13.0154 - val_accuracy: 0.0623\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 4.48218\n",
            "Epoch 20/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0104 - accuracy: 0.9986 - val_loss: 17.8642 - val_accuracy: 0.0539\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 4.48218\n",
            "Epoch 21/100\n",
            "336/336 [==============================] - 12s 34ms/step - loss: 0.0261 - accuracy: 0.9980 - val_loss: 13.8481 - val_accuracy: 0.0599\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 4.48218\n",
            "Epoch 22/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0296 - accuracy: 0.9978 - val_loss: 13.5505 - val_accuracy: 0.0551\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 4.48218\n",
            "Epoch 23/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0182 - accuracy: 0.9984 - val_loss: 12.2301 - val_accuracy: 0.0539\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 4.48218\n",
            "Epoch 24/100\n",
            "336/336 [==============================] - 12s 34ms/step - loss: 0.0167 - accuracy: 0.9982 - val_loss: 14.3324 - val_accuracy: 0.0551\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 4.48218\n",
            "Epoch 25/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0103 - accuracy: 0.9990 - val_loss: 12.8585 - val_accuracy: 0.0647\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 4.48218\n",
            "Epoch 26/100\n",
            "336/336 [==============================] - 12s 34ms/step - loss: 0.0129 - accuracy: 0.9989 - val_loss: 12.8969 - val_accuracy: 0.0527\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 4.48218\n",
            "Epoch 27/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0174 - accuracy: 0.9981 - val_loss: 13.5122 - val_accuracy: 0.0479\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 4.48218\n",
            "Epoch 28/100\n",
            "336/336 [==============================] - 12s 34ms/step - loss: 0.0119 - accuracy: 0.9988 - val_loss: 10.8115 - val_accuracy: 0.0539\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 4.48218\n",
            "Epoch 29/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0154 - accuracy: 0.9989 - val_loss: 9.8436 - val_accuracy: 0.0515\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 4.48218\n",
            "Epoch 30/100\n",
            "336/336 [==============================] - 12s 34ms/step - loss: 0.0158 - accuracy: 0.9974 - val_loss: 15.0064 - val_accuracy: 0.0563\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 4.48218\n",
            "Epoch 31/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0194 - accuracy: 0.9980 - val_loss: 12.4502 - val_accuracy: 0.0515\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 4.48218\n",
            "Epoch 32/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0120 - accuracy: 0.9986 - val_loss: 11.2741 - val_accuracy: 0.0515\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 4.48218\n",
            "Epoch 33/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0143 - accuracy: 0.9988 - val_loss: 10.9833 - val_accuracy: 0.0527\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 4.48218\n",
            "Epoch 34/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0104 - accuracy: 0.9988 - val_loss: 10.2065 - val_accuracy: 0.0419\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 4.48218\n",
            "Epoch 35/100\n",
            "336/336 [==============================] - 12s 34ms/step - loss: 0.0096 - accuracy: 0.9983 - val_loss: 13.4910 - val_accuracy: 0.0455\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 4.48218\n",
            "Epoch 36/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0060 - accuracy: 0.9991 - val_loss: 12.2726 - val_accuracy: 0.0479\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 4.48218\n",
            "Epoch 37/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0069 - accuracy: 0.9990 - val_loss: 12.6881 - val_accuracy: 0.0431\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 4.48218\n",
            "Epoch 38/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0052 - accuracy: 0.9997 - val_loss: 10.8130 - val_accuracy: 0.0431\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 4.48218\n",
            "Epoch 39/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0054 - accuracy: 0.9996 - val_loss: 10.0308 - val_accuracy: 0.0503\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 4.48218\n",
            "Epoch 40/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0054 - accuracy: 0.9993 - val_loss: 14.0775 - val_accuracy: 0.0347\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 4.48218\n",
            "Epoch 41/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 12.5180 - val_accuracy: 0.0551\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 4.48218\n",
            "Epoch 42/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0093 - accuracy: 0.9988 - val_loss: 14.8229 - val_accuracy: 0.0467\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 4.48218\n",
            "Epoch 43/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0131 - accuracy: 0.9983 - val_loss: 15.0241 - val_accuracy: 0.0395\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 4.48218\n",
            "Epoch 44/100\n",
            "336/336 [==============================] - 12s 34ms/step - loss: 0.0218 - accuracy: 0.9978 - val_loss: 11.8271 - val_accuracy: 0.0455\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 4.48218\n",
            "Epoch 45/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0085 - accuracy: 0.9991 - val_loss: 12.4821 - val_accuracy: 0.0491\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 4.48218\n",
            "Epoch 46/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0146 - accuracy: 0.9989 - val_loss: 12.2937 - val_accuracy: 0.0479\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 4.48218\n",
            "Epoch 47/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0096 - accuracy: 0.9987 - val_loss: 11.3597 - val_accuracy: 0.0515\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 4.48218\n",
            "Epoch 48/100\n",
            "336/336 [==============================] - 12s 35ms/step - loss: 0.0236 - accuracy: 0.9983 - val_loss: 14.2717 - val_accuracy: 0.0515\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 4.48218\n",
            "Epoch 49/100\n",
            " 11/336 [..............................] - ETA: 10s - loss: 0.0048 - accuracy: 0.9986"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDGXJUAnIlQy"
      },
      "source": [
        "Once our model is trained, we load the weights that were saved earlier, and use it to run the model on our test data to evaluate the accuracy of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq5NU1GfZbNC"
      },
      "source": [
        "model.load_weights('saved_models/weights.best.from_scratch.hdf5')\n",
        "# get index of predicted dog breed for each image in test set\n",
        "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
        "# report test accuracy\n",
        "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
        "print('Test accuracy: %.4f%%' % test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqtIaBKVIzPi"
      },
      "source": [
        "This model works decent and will give an accuracy of ~7%. You must be wondering, so much to get such low accuracy? Remember, this is without any parameter fine-tuning and data augmentation. This is where the next steps will help improve accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6pPcn0hI3z4"
      },
      "source": [
        "# Step 5: Use pre-built Keras CNN models and modify those to Classify Dog Breeds (using Transfer Learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et7aaD46JAwj"
      },
      "source": [
        "There are some groundbreaking pre-built CNN architectures, available in Keras that could be used through transfer learning viz. VGG16, VGG19, ResNet50, Xception, InceptionV3. These models help reduce training time without sacrificing accuracy. Here the pre-trained VGG-16 model has been used and is fed into our model. We only added a global average pooling layer (to reduce dimensionality) and a fully connected layer with a softmax activation function (to get one node for each dog category)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqMR_TNdUel8"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTgO0LXoZoSa"
      },
      "source": [
        "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
        "train_VGG16 = bottleneck_features['train']\n",
        "valid_VGG16 = bottleneck_features['valid']\n",
        "test_VGG16 = bottleneck_features['test']\n",
        "# CNN architecture using Transfer Learning\n",
        "VGG16_model = Sequential()\n",
        "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
        "VGG16_model.add(Dense(133, activation='softmax'))\n",
        "VGG16_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAaqstYbJJZx"
      },
      "source": [
        "When we ran our test data through this newly trained model and pre-computed features, our accuracy increased to ~45% in less duration which is a significant improvement. This is because now there are only 2 layers in the network that are being processed. The accuracy further jumped to ~82% with ResNet50 model, which is what I ended up using in my code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGlLlDACJPOc"
      },
      "source": [
        "# Step 6: Write an Algorithm to bind the steps above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_wotIhMJWA8"
      },
      "source": [
        "This is the step where we put all the different pieces together. We write a simple algorithm that accepts an image path and first determines whether it contains a face of human, dog, or neither. Then,\n",
        "if a dog is detected in the image, return the predicted breed.\n",
        "if a human is detected in the image, return the resembling dog breed.\n",
        "if neither is detected in the image, provide output that indicates an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBuv5jk1JRWS"
      },
      "source": [
        "def display_detect_image(img_path):\n",
        "    detect_breed(img_path)\n",
        "    # load color (BGR) image\n",
        "    img = cv2.imread(img_path)\n",
        "    # convert BGR image to RGB for plotting\n",
        "    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    # display the image\n",
        "    plt.imshow(cv_rgb)\n",
        "    return plt.show()\n",
        "def detect_breed(img_path):\n",
        "    # check if image is human face\n",
        "    if face_detector(img_path) == True:\n",
        "        return print(\"Hello human! Your face resembles a: \",Resnet50_predict_breed(img_path).str.split(\".\")[-1])\n",
        "    # check if image is dog face\n",
        "    elif dog_detector(img_path) == True:\n",
        "        return print(\"Hello dog! Your predicted breed is: \",Resnet50_predict_breed(img_path).str.split(\".\")[-1])\n",
        "    # else print an error message\n",
        "    else:\n",
        "        return print(\"Oops! This is neither a human nor a dog\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7GYuUOeJaf0"
      },
      "source": [
        "# Step 7: Test our Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8qw_UFRJexB"
      },
      "source": [
        "This is the section where we see the power of our algorithm and take it for a spin! I supplied some random dog, and human images and voila! the algorithm predicts the breed. Now, if you like a dog on a street or in a park, and you want to know its breed, no need to ask the owner, just click a picture and run it through the mode"
      ]
    }
  ]
}